topology:
  hosts:
    - james
    - mike
    - s17
    - s18

model:
  repo_id: mlx-community/Llama-3.2-1B-Instruct-4bit
  # repo_id: mlx-community/Llama-3.2-1B-Instruct-bf16
  
  output_location: ./model_output/

  lora:
    num_layers: 16
    rank: 8
    scale: 16
    dropout: 0.0

  # architecture: "llama"
  # dimensions:
  #   hidden_size: 128
  #   intermediate_size: 256  # 384 * 4
  #   num_layers: 4
  # attention:
  #   num_heads: 8
  # normalization:
  #   rms_norm_eps: 1.0e-5
  # rope:
  #   theta: 10000
  #   traditional: false
  #   scaling: null
  # misc:
  #   attention_bias: false
  #   mlp_bias: false
  #   tie_word_embeddings: true

optimizer:
  learning_rate: 0.0002
  eps: 0.0001

dataset:
  path: "./data/train.jsonl"
  batch_size: 8
  max_seq_length: 1024
  epochs: 1 
