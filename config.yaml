topology:
  hosts:
    - james
    - mike
    - s17
    - s18

model:
  # repo_id: mlx-community/Llama-3.2-1B-Instruct-4bit
  # repo_id: mlx-community/Llama-3.2-1B-Instruct-bf16
  # repo_id: mlx-community/Llama-3.3-70B-Instruct-4bit
  repo_id: mlx-community/DeepSeek-V3.1-8bit

  output_location: ./model_output/

  # lora:
  #   num_layers: 61
  #   rank: 8
  #   scale: 16
  #   dropout: 0.0

  grad_checkpoint: 'medium'
  # grad_checkpoint: 'none'

  # distributed: 'dp'
  distributed: 'ppp'

  auto_parallel:
    enabled: true

  # architecture: "llama"
  # dimensions:
  #   hidden_size: 128
  #   intermediate_size: 256  # 384 * 4
  #   num_layers: 4
  # attention:
  #   num_heads: 8
  # normalization:
  #   rms_norm_eps: 1.0e-5
  # rope:
  #   theta: 10000
  #   traditional: false
  #   scaling: null
  # misc:
  #   attention_bias: false
  #   mlp_bias: false
  #   tie_word_embeddings: true

optimizer:
  learning_rate: 0.00005
  # learning_rate: 0.001
  eps: 0.0001

dataset:
  path: "./data/train.jsonl"
  batch_size: 16
  # batch_size: 16
  max_seq_length: 1024
  epochs: 1
